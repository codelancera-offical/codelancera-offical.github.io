{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "np.random.seed(50)\n",
    "\n",
    "# 随机生成三类数据，每类20个样本，数据维度为3, 并设置分别的中心点\n",
    "class1 = np.random.randn(20, 3) + [1, 1, 1]\n",
    "class2 = np.random.randn(20, 3) + [4, 2, 1]\n",
    "class3 = np.random.randn(20, 3) + [5, 2, 9]\n",
    "\n",
    "X = np.vstack((class1, class2, class3)) # 按行堆叠数据\n",
    "y = np.array([0]*20 + [1]*20 + [2]*20)  # 指定每个样本的类别标签\n",
    "\n",
    "# 划分训练验证集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "\n",
    "# 独热编码 \n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1)) \n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基类层\n",
    "class Layer:\n",
    "\n",
    "\t# 前向传播函数，根据上一层输入x计算\n",
    "\tdef forward(self, x):\n",
    "\t\traise NotImplementedError # 未实现错误\n",
    "\n",
    "\t# 反向传播函数，输入下一层回传的梯度grad, 输出当前层的梯度\n",
    "\tdef backward(self, grad):\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\t# 更新函数，用于更新当前层的参数\n",
    "\tdef update(self, learning_rate):\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "\tdef __init__(self, num_in, num_out, use_bias=True):\n",
    "\t\tself.num_in = num_in # 输入维度\n",
    "\t\tself.num_out = num_out # 输出维度\n",
    "\t\tself.use_bias = use_bias # 是否添加偏置\n",
    "\t\n",
    "\t\t# 参数的初始化（绝对不能初始化为0！不然后续计算拾取意义）\n",
    "\t\t# 用正态分布来初始化W\n",
    "\t\tself.W = np.random.normal(loc=0, scale=1.0, size=(num_out, num_in))\n",
    "\t\tif use_bias:\n",
    "\t\t\tself.b = np.zeros((1, num_out))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# 前向传播 y = Wx + b\n",
    "\t\t# x的维度为(batch_size#批次处理, num_in)\n",
    "\t\tself.x = x\n",
    "\t\tself.y = x @ self.W.T # y的维度为(batch_size, num_out)\n",
    "\t\tif self.use_bais:\n",
    "\t\t\tself.y += self.b\n",
    "\n",
    "\t\treturn self.y\n",
    "\n",
    "\tdef backward(self, grad):\n",
    "\t\t# 反向传播，按照链式法则计算\n",
    "\t\t# grad的维度为(batch_size, num_out)\n",
    "\t\t# 梯度应该对batch_size去平均值\n",
    "\t\t# grad_W的维度应该与W相同，为(num_in, num_out)\n",
    "\t\tself.grad_W = self.x.T @ grad / grad.shape[0]\n",
    "\t\tif self.use_bias:\n",
    "\t\t\t# grab_b的维度与b相同，(1, num_out)\n",
    "\t\t\tself.grad_b = np.mean(grad, axis = 0, keepdims = True) # 对 grad 沿批次维度（行）取平均值，并保留维度信息以确保结果形状与偏置向量 b 一致。\n",
    "\t\t# 往上一层传递的grad维度应该为(batch_size, num_in)\n",
    "\t\tgrad = grad @ self.W.T\n",
    "\t\treturn grad\n",
    "\n",
    "\tdef update(self, learning_rate):\n",
    "\t\t# 更新参数以完成梯度下降\n",
    "\t\tself.W -= learning_rate * self.grad_W\n",
    "\t\tif self.use_bais:\n",
    "\t\t\tself.b -= learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Layer):\n",
    "\t# 啥都不动层\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x\n",
    "\tdef backward(self, grad):\n",
    "\t\treturn grad\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "\t# Sigmoid激活层\n",
    "\tdef forward(self, x):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = 1 / (1 + np.exp(-x))\n",
    "\t\treturn self.y\n",
    "\n",
    "\tdef backward(self, grad):\n",
    "\t\treturn grad * self.y * (1 - self.y)\n",
    "\n",
    "class Tanh(Layer):\n",
    "\t# Tanh激活层\n",
    "\tdef forward(self, x):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = np.tanh(x)\n",
    "\t\treturn self.y\n",
    "\n",
    "\tdef backward(self, grad):\n",
    "\t\treturn grad * (1 - self.y ** 2)\n",
    "\n",
    "class ReLU(Layer):\n",
    "\t# Relu激活层\n",
    "\tdef forward(self, x):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = np.maxinum(x, 0)\n",
    "\t\treturn self.y\n",
    "\n",
    "\tdef backward(self, grad):\n",
    "\t\treturn grad * (self.x >= 0)\n",
    "\n",
    "class Softmax: \n",
    "\tdef forward(self, x): \n",
    "\t\texp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) \n",
    "\t\treturn exp_x / np.sum(exp_x, axis=1, keepdims=True) \n",
    "\t\n",
    "\tdef backward(self, grad): \n",
    "\t\treturn grad\n",
    "\n",
    "# 存储所有激活函数和对应名称，方便索引\n",
    "activation_dict = {\n",
    "\t'identity' : Identity,\n",
    "\t'sigmoid' : Sigmoid,\n",
    "\t'tanh' : Tanh,\n",
    "\t'relu' : ReLU,\n",
    "\t'softmax': Softmax\n",
    "}\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tlayer_sizes, # 包含每层大小的list\n",
    "\t\tuse_bias = True,\n",
    "\t\tactivation='sigmoid',\n",
    "\t\tout_activation='softmax'\n",
    "\t):\n",
    "\t\tself.layers = []\n",
    "\t\tnum_in = layer_sizes[0]\n",
    "\t\tfor num_out in layer_sizes[1:-1]:\n",
    "\t\t\t# 添加全连接层\n",
    "\t\t\tself.layers.append(Linear(num_in, num_out, use_bias))\n",
    "\t\t\t# 添加激活函数\n",
    "\t\t\tself.layers.append(activation_dict[activation]())\n",
    "\t\t\tnum_in = num_out\n",
    "\t\t# 最后一层特殊处理\n",
    "\t\tself.layers.append(Linear(num_in, layer_sizes[-1], use_bias))\n",
    "\t\tself.layers.append(activation_dict[out_activation]())\n",
    "\n",
    "\tdef forward(self, grad):\n",
    "\t\t# 前向传播，将输入依次通过每一层\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer.forward(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef backward(self, grad):\n",
    "\t\t# 反向传播, grad为损失函数对输出的梯度，将该梯度依次回传，的到每一层参数的梯度\n",
    "\t\tfor layer in reversed(self.layers):\n",
    "\t\t\tgrad = layer.backward(grad)\n",
    "\n",
    "\tdef update(self, learning_rate):\n",
    "\t\t# 更新每一层的参数\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tlayer.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'x' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train_onehot[st:ed]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 计算MLP的预测\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mforward(x_batch)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m     30\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(y_pred \u001b[38;5;241m+\u001b[39m eps) \u001b[38;5;241m*\u001b[39m y_batch) \u001b[38;5;241m/\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, grad):\n\u001b[0;32m     22\u001b[0m \t\u001b[38;5;66;03m# 前向传播，将输入依次通过每一层\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 24\u001b[0m \t\tx \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     25\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'x' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 3000\n",
    "learning_rate = 0.05\n",
    "batch_size = 20\n",
    "eps = 1e-7 # 用于防止除0，log（0）等问题\n",
    "\n",
    "# 创建一个层大小依次为3, 8, 3的多层感知机\n",
    "# 对于多分类问题，使用softmax作为输出层的激活函数\n",
    "mlp = MLP(layer_sizes=[3, 8, 3], use_bias=True, activation='relu', out_activation='softmax')\n",
    "\n",
    "# 记录损失和准确率 \n",
    "train_losses = [] \n",
    "test_losses = [] \n",
    "train_accuracies = [] \n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\tst = 0\n",
    "\tloss = 0.0\n",
    "\twhile st < len(X_train):\n",
    "\t\ted = min(st + batch_size, len(X_train))\n",
    "\t\t# 取出batch\n",
    "\t\tx_batch = X_train[st:ed]\n",
    "\t\ty_batch = y_train_onehot[st:ed]\n",
    "\t\t# 计算MLP的预测\n",
    "\t\ty_pred = mlp.forward(x_batch)\n",
    "\t\t# 计算损失\n",
    "\t\tbatch_loss = -np.sum(np.log(y_pred + eps) * y_batch) / y_batch.shape[0]\n",
    "\t\tloss += batch_loss\n",
    "\t\t# 计算梯度并进行反向传播\n",
    "\t\tgrad = y_pred - y_batch\n",
    "\t\tmlp.backward(grad)\n",
    "\t\t# 更新参数\n",
    "\t\tmlp.update(learning_rate)\n",
    "\t\tst = ed\n",
    "\tloss /= (len(X_train) / batch_size)\n",
    "\ttrain_losses.append(loss)\n",
    "\t# 计算训练准确率\n",
    "\ttrain_acc = np.mean(np.argmax(mlp.forward(X_train), axis=1) == y_train)\n",
    "\ttrain_accuracies.append(train_acc)\n",
    "\t# 计算测试损失和准确率\n",
    "\ttest_loss = -np.sum(np.log(mlp.forward(X_test) + eps) * y_test_onehot) / y_test_onehot.shape[0]\n",
    "\ttest_losses.append(test_loss)\n",
    "\ttest_acc = np.mean(np.argmax(mlp.forward(X_test), axis=1) == y_test)\n",
    "\ttest_accuracies.append(test_acc)\n",
    "\tif epoch % 100 == 0:\n",
    "\t\tprint(f'Epoch {epoch}, Train Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# 可视化训练和测试的损失与准确率\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
